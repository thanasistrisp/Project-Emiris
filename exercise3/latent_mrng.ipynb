{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optimization_latent notebook cells to be copied to original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "from helper_funcs import *\n",
    "\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_pareto_front, plot_optimization_history, plot_slice\n",
    "\n",
    "from params import lsh_test, hypercube_test, kmeans_test, gnn_test, mrng_test, nsg_test, get_aaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir('./models/')\n",
    "\n",
    "dataset = b'MNIST/input.dat'\n",
    "query   = b'MNIST/query.dat'\n",
    "\n",
    "model_to_files = {}\n",
    "for i, model in enumerate(models):\n",
    "    normalized_dataset = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_dataset.dat'\n",
    "    normalized_query   = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_query.dat'\n",
    "    encoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_dataset.dat'\n",
    "    encoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_query.dat'\n",
    "    decoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_dataset.dat'\n",
    "    decoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_query.dat'\n",
    "\n",
    "    model_to_files.update({models[i] : [normalized_dataset, normalized_query,\n",
    "                                        encoded_dataset, encoded_query,\n",
    "                                        decoded_dataset, decoded_query]})\n",
    "\n",
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_to_files:\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    model = b'models/' + model.encode()\n",
    "\n",
    "    # load model\n",
    "    autoencoder = load_model(model.decode())\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # get shape of encoded layer\n",
    "\n",
    "    # load dataset\n",
    "    x_train = load_dataset(dataset)\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = load_dataset(query)\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    if len(shape) == 3: # if model type is convolutional\n",
    "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "    else:\n",
    "        x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "    encoded_train = autoencoder.encode(x_train)\n",
    "    encoded_test = autoencoder.encode(x_test)\n",
    "\n",
    "    # deflatten encoded datasets\n",
    "    encoded_train = deflatten_encoded(encoded_train, shape)\n",
    "    encoded_test = deflatten_encoded(encoded_test, shape)\n",
    "\n",
    "    # decode encoded datasets\n",
    "    decoded_train = autoencoder.decode(encoded_train)\n",
    "    decoded_test = autoencoder.decode(encoded_test)\n",
    "\n",
    "    # save original datasets normalized\n",
    "    save_decoded_binary(x_train, normalized_dataset)\n",
    "    save_decoded_binary(x_test, normalized_query)\n",
    "\n",
    "    # normalize encoded datasets\n",
    "    encoded_train = normalize(encoded_train)\n",
    "    encoded_test = normalize(encoded_test)\n",
    "\n",
    "    # save encoded datasets\n",
    "    save_encoded_binary(encoded_train, encoded_dataset)\n",
    "    save_encoded_binary(encoded_test, encoded_query)\n",
    "\n",
    "    # normalize decoded datasets\n",
    "    decoded_train = normalize(decoded_train)\n",
    "    decoded_test = normalize(decoded_test)\n",
    "\n",
    "    # save decoded datasets\n",
    "    save_decoded_binary(decoded_train, decoded_dataset)\n",
    "    save_decoded_binary(decoded_test, decoded_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mrng(trial):\n",
    "    model = trial.suggest_categorical('model', model_to_files.keys())\n",
    "    param_dict = {'l': trial.suggest_int('l', 1, 1000)}\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    encoded_dataset, encoded_query = model_to_files[model][2:4]\n",
    "\n",
    "    average_time, aaf_latent = mrng_test(encoded_dataset, encoded_query, queries_num=100, **param_dict, N=5, int_data=0)\n",
    "\n",
    "    return aaf_latent.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mrng_study = optuna.create_study(study_name='mrng', directions=['minimize', 'minimize'])\n",
    "mrng_study.optimize(objective_mrng, n_trials=50, n_jobs=3)\n",
    "print(\"-------------------- Best trials --------------------\")\n",
    "trials = sorted(mrng_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mrng_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(mrng_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(mrng_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(mrng_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(mrng_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(mrng_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
