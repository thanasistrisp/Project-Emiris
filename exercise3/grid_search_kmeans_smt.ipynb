{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "from helper_funcs import *\n",
    "\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "from params import kmeans_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir('./models/')\n",
    "\n",
    "dataset = b'MNIST/input.dat'\n",
    "query   = b'MNIST/query.dat'\n",
    "\n",
    "model_to_files = {}\n",
    "for i, model in enumerate(models):\n",
    "    normalized_dataset = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_dataset.dat'\n",
    "    normalized_query   = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_query.dat'\n",
    "    encoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_dataset.dat'\n",
    "    encoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_query.dat'\n",
    "    decoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_dataset.dat'\n",
    "    decoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_query.dat'\n",
    "\n",
    "    model_to_files.update({models[i] : [normalized_dataset, normalized_query,\n",
    "                                        encoded_dataset, encoded_query,\n",
    "                                        decoded_dataset, decoded_query]})\n",
    "\n",
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_to_files:\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    model = b'models/' + model.encode()\n",
    "\n",
    "    # load model\n",
    "    autoencoder = load_model(model.decode())\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # get shape of encoded layer\n",
    "\n",
    "    # load dataset\n",
    "    x_train = load_dataset(dataset)\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = load_dataset(query)\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    if len(shape) == 3: # if model type is convolutional\n",
    "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "    else:\n",
    "        x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "    encoded_train = autoencoder.encode(x_train)\n",
    "    encoded_test = autoencoder.encode(x_test)\n",
    "\n",
    "    # deflatten encoded datasets\n",
    "    encoded_train = deflatten_encoded(encoded_train, shape)\n",
    "    encoded_test = deflatten_encoded(encoded_test, shape)\n",
    "\n",
    "    # decode encoded datasets\n",
    "    decoded_train = autoencoder.decode(encoded_train)\n",
    "    decoded_test = autoencoder.decode(encoded_test)\n",
    "\n",
    "    # save original datasets normalized\n",
    "    save_decoded_binary(x_train, normalized_dataset)\n",
    "    save_decoded_binary(x_test, normalized_query)\n",
    "\n",
    "    # normalize encoded datasets\n",
    "    encoded_train = normalize(encoded_train)\n",
    "    encoded_test = normalize(encoded_test)\n",
    "\n",
    "    # save encoded datasets\n",
    "    save_encoded_binary(encoded_train, encoded_dataset)\n",
    "    save_encoded_binary(encoded_test, encoded_query)\n",
    "\n",
    "    # normalize decoded datasets\n",
    "    decoded_train = normalize(decoded_train)\n",
    "    decoded_test = normalize(decoded_test)\n",
    "\n",
    "    # save decoded datasets\n",
    "    save_decoded_binary(decoded_train, decoded_dataset)\n",
    "    save_decoded_binary(decoded_test, decoded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lsh = [4, 7, 0.6, 1]            # L, k, window, limit_queries\n",
    "best_params_hypercube = [67, 3, 1000, 0.42] # M, k, probes, window\n",
    "\n",
    "print_lock = Lock()\n",
    "rows_lock = Lock()\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(clustering_time, stotal_latent, silhouette):\n",
    "    print_lock.acquire()\n",
    "    print(\"Clustering time :\", clustering_time)\n",
    "    print(\"Total silhouette:\", stotal_latent)\n",
    "    print(\"Silhouette per cluster:\", silhouette)\n",
    "    print(\"-------------------------\")\n",
    "    print_lock.release()\n",
    "\n",
    "def rows_append(row):\n",
    "    rows_lock.acquire()\n",
    "    rows.append(row)\n",
    "    rows_lock.release()\n",
    "\n",
    "def run_kmeans_classic(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    config = {\n",
    "        'model': b'CLASSIC',\n",
    "        'vals': [],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    clustering_time, stotal_latent, silhouette = kmeans_test(conf=config, int_data=0)\n",
    "\n",
    "    print_results(clustering_time.val, stotal_latent.val, silhouette.val)\n",
    "\n",
    "    rows_append([model, 'CLASSIC', clustering_time.value, stotal_latent.value, silhouette.val])\n",
    "\n",
    "def run_kmeans_lsh(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    config = {\n",
    "        'model': b'LSH',\n",
    "        'vals': best_params_lsh[:-2].append(best_params_lsh[-1]),\n",
    "        'window': best_params_lsh[-2],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    clustering_time, stotal_latent, silhouette = kmeans_test(conf=config, int_data=0)\n",
    "\n",
    "    print_results(clustering_time.val, stotal_latent.val, silhouette.val)\n",
    "\n",
    "    rows_append([model, 'LSH', clustering_time.value, stotal_latent.value, silhouette.val])\n",
    "\n",
    "def run_kmeans_cube(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    config = {\n",
    "        'model': b'LSH',\n",
    "        'vals': best_params_hypercube[:-1],\n",
    "        'window': best_params_hypercube[-1],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    clustering_time, stotal_latent, silhouette = kmeans_test(conf=config, int_data=0)\n",
    "\n",
    "    print_results(clustering_time.val, stotal_latent.val, silhouette.val)\n",
    "\n",
    "    rows_append([model, 'CUBE', clustering_time.value, stotal_latent.value, silhouette.val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    t_classic = threading.Thread(target=run_kmeans_classic, args=model)\n",
    "    t_lsh     = threading.Thread(target=run_kmeans_lsh, args=model)\n",
    "    t_cube    = threading.Thread(target=run_kmeans_cube, args=model)\n",
    "\n",
    "    t_classic.start()\n",
    "    t_lsh.start()\n",
    "    t_cube.start()\n",
    "\n",
    "    t_classic.join()\n",
    "    t_lsh.join()\n",
    "    t_cube.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_models, col_methods, col_clustering_time, col_stotal_latent = [], [], [], []\n",
    "\n",
    "for row in rows:\n",
    "    model, method, clustering_time, stotal_latent, silhouette = row\n",
    "\n",
    "    col_models.append(model)\n",
    "    col_methods.append(method)\n",
    "    col_clustering_time.append(clustering_time)\n",
    "    col_stotal_latent.append(stotal_latent)\n",
    "\n",
    "col_dict = {'model': col_models, 'method': col_methods,\n",
    "            'clustering time': col_clustering_time, 'silhouette total': col_stotal_latent}\n",
    "\n",
    "df = pandas.DataFrame(data=col_dict)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
