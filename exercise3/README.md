# Software Development for Algorithmic Problems - Winter Semester 2023-24

# Project 3 - Dimensionality reduction via Neural Network and experimental study

Eleftheria Vrachoriti - 1115202000026

Athanasios Trispiotis - 1115202000194

# Table of Contents
- [1. Project structure](#1-project-structure)
- [2. Compilation](#2-compilation)
	- [2.1. Shared library `shared_lib.so`](#21-shared-library-shared_libso)
	- [2.2. Clean](#22-clean)
- [3. Execution](#3-execution)
	- [3.1. Python scripts](#31-python-scripts)
		- [3.1.1. `reduce.py`](#311-reducepy)
	- [3.2. C++ binaries](#32-c-binaries)
- [4. Neural Network: Autoencoder](#4-neural-network-autoencoder)
	- [4.1. Architecture](#41-architecture)
		- [4.1.1. Dense Autoencoder](#411-dense-autoencoder)
		- [4.1.2. Convolutional Autoencoder](#412-convolutional-autoencoder)
	- [4.2. Normalization of datasets](#42-normalization-of-datasets)
- [5. Nearest Neighbor Search in Latent Space](#5-nearest-neighbor-search-in-latent-space)
	- [5.1. Implementation - Evaluation](#51-implementation---evaluation)
	- [5.2 Results](#52-results)
- [6. Clustering in Latent Space](#6-clustering-in-latent-space)
	- [6.1. Implementation - Evaluation](#61-implementation---evaluation)
	- [6.2 Results](#62-results)
- [References](#references)


# 1. Project structure

```bash
├── . (exercise3)		# directory for source code of the third (current) assignment
│   ├── lib				# directory for shared library
│   │   └── Makefile		# makefile for building only the shared library
│   │
│   ├── MNIST			# directory for MNIST binary data files (input.dat, query.dat, encoded, decoded, normalized binary files generated by python)
│   │   ├── input.dat
│   │   └── query.dat
│   │
│   ├── models			# directory for saved already trained autoencoder models (best selected models)
│   │   ├── model_conv_12.keras
│   │   ├── model_conv_19.keras
│   │   ├── model_conv_46.keras
│   │   ├── model_dense_1.keras
│   │   ├── model_dense_26.keras
│   │   └── model_dense_43.keras
│   │
│   ├── python_cpp_connection		# directory for python-cpp connection files
│   │   ├── clustering.cc				# connector functions for calculating silhouette metric in initial and latent space
│   │   ├── config.hpp
│   │   ├── kmeans_eval.cc
│   │   ├── kmeans_eval.hpp
│   │   └── nearest_neighbor.cc			# connector functions for calculating AAF metrics (two versions)
│   │
│   ├── PDFs			# directory containing the generated PDFs from the jupyter notebooks
│   │
│   ├── autoencoder.py		# definition of the autoencoder class (Keras model)
│   ├── best_model_analysis.ipynb
│   ├── common.mk			# main (common) makefile
│   ├── environment.yml		# conda environment file for installing dependencies
│   ├── grid_search_kmeans_latent.ipynb
│   ├── grid_search_kmeans_lat_init.ipynb
│   ├── grid_search_lat_init.ipynb
│   ├── helper_funcs.py		# helper functions for loading/saving data, normalizing, plotting, numpy handling
│   ├── optimization_initial.ipynb
│   ├── optimization_latent.ipynb
│   ├── params.py			# python functions calling the cpp functions using ctypes (sihlouette, AAF metrics)
│   ├── README.md			# this documentation file
│   └── reduce.py			# python requested file for generating encoded normalized binary files
│
├── ../exercise1/		# directory for source code of the first assignment
│   └── ...
│
└── ../exercise2/		# directory for source code of the second assignment
	└── ...
```

# 2. Compilation

## 2.1. Shared library `shared_lib.so`

Stay in the root directory of the <code>exercise3/</code> and then run the following command:

```bash
make -C lib
```

This will build the shared library <code>shared_lib.so</code> in the <code>lib/</code> directory.

Note: For GNNs, MRNG and NSG, the construction will be done based on best parameters found in <code>exercise2/include/defines_latent_space.hpp</code> and <code>exercise2/include/defines_initial_space.hpp</code> files. By default, compilation will be done for latent space. If you want to compile for initial space, you will have to comment out the shared_lib.so_FLAGS line in the Makefile and recompile (clean and rebuild) the library.

## 2.2. Clean

To remove dependency, object, shared library and executable files, run the following command at `lib/` directory:

```bash
make clean
```

# 3. Execution

## 3.1. Python scripts

### 3.1.1. `reduce.py`

This script is used to generate the encoded normalized binary files for the MNIST dataset. It is called as follows:

```bash
python reduce.py -d <dataset> -q <queryset> -od <output_dataset_file> -oq <output_query_file> [-m <keras_model>]
```
By default, one of the best models is used for encoding. If you want to use a specific model, you can specify it with the `-m` flag from `models/` directory.

## 3.2. C++ binaries

For this exercise, there are no C++ binaries to be executed. Only the shared library is used through ctypes python library. If you want to measure various metrics on other datasets, you will have to run the equivalent cells in the Jupyter notebooks and adjust them appropriately (e.g. change paths, parameters, run one model/dataset only bypassing hyperparameter tuning, etc.).

# 4. Neural Network: Autoencoder

For developing our Autoencoder model, we will use the Keras framework with TensorFlow backend.

## 4.1. Architecture

Our unsupervised learning model will always have as input 784 neurons (28x28 pixels) and the output layer will have the dimension of the latent space, always less than 50 to be comparably smaller than initial dimension. We will use up to 5 hidden connected layers. Other parameters are explained inside notebook, which we evaluate by using random search with Optuna framework for the validation loss. Overfitting is also handled by using early stopping and manual observation of the training and validation loss in each epoch. Epochs are set to max 50 epochs for all models.

### 4.1.1. Dense Autoencoder

We start as a stack of fully-connected neural layers (a linear operation in which every input is connected to every output by a weight followed by a non-linear activation function). As we can see from the best trials Optuna returns, no model overfits. The least validation loss we can yield is about 0.075. For the best models, relu or gelu activation functions and nadam or adamax optimizers are the most appropriate for the hidden layers and sigmoid for the output layer (classification problem). For the batch size, the model behaves better with smaller batch sizes (32,64). Simpler models with 1-2 hidden layers are better than more complex ones. Early stopping does not happen in most of the cases as epochs are not too many and the model does not overfit. As expected, better validation loss is achieved with higher latent space dimensionality.

### 4.1.2. Convolutional Autoencoder

We also tried convolutional layers for our NN (a linear operation using a subset of the weights of a dense layer). As we can see from the best trials, the least validation loss with latent space dimensionality less than 50 is about 0.08. Here for better results we need more layers (about 4).
We use kernel size $3\times3$, pool size $2\times2$, stride $1\times1$ and padding $same$. Filters size varies from 8 to 64.
For the other hyperparameters, they are similar to the dense autoencoder, except that the model behaves better with larger batch sizes (128,256). We achieve better result with the encoded representations of $(1,1,25), (2,2,8), (1,1,18)$.

Even though the convolutional autoencoder is more complex than the dense one, it does not perform better. In general, when the inputs are images, convolutional neural networks (CNNs) are considered the best architecture for encoding. This is not our case here, as MNIST dataset is not complex enough and the latent dimension is too small to be able to fully exploit them (they perform better with latent dimension ~150 with about 0.07 loss).

## 4.2. Normalization of datasets

We normalize all datasets (original, encoded) for both training and testing sets to be in interval $[0,1]$. It is necessary for the data to be in the same scale to be able to compare distances proportionally, else the results will be biased. Normalization in same space does not affect the results. Normalization is done in python and data are saved as float32 in binary files (equivalent type with C++ float).

# 5. Nearest Neighbor Search in Latent Space

## 5.1. Implementation - Evaluation

To be able to evaluate nearest neighbor search in latent space to see how the model performs in latent space, we need to project the "encoded" point (approximate neighbor to encoded query) back to initial space and compare it with the true neighbor to query in initial space. For this evaluation, we calculate the fraction:
$$
	AAF_{average\_over\_all\_queries} = \frac{p_{approximate\_neighbor_{projected}} - q_{initial}}{p_{{true\_nearest\_neighbor}_{initial}} - q_{initial}}
$$

For calculating approximate nearest neighbor in latent space, we need to reevaluate the hyperparameters for each algorithm as the previous hyperparameters do not perform well in significantly lower dimension space. We also calculate true nearest neighbor in latent space for evaluation purposes.

To be able to calculate nearest neighbor (nn) search in latent space, we need to encode whole dataset and query sets using the `encode` property of the model. Then, we save them to binary files as float32 in interval $[0,1]$ and load them back to C++. We can project nearest neighbor encoded back to initial space just by using its index in the initial dataset, as datasets are not shuffled.

## 5.2 Results

Analysis is done in 3 notebooks:

- `optimization_initial.ipynb`: recalculate the best hyparameters, AAF and time for initial space using AAF (average approximation factor) instead of MAF metric.
- `optimization_latent.ipynb`: calculate the best hyparameters, AAF and time only for latent space. AAF evaluation here does not make sense as we do not project the points back to initial space except for the hyperparameter tuning.
- `grid_search_lat_init.ipynb`: using the best hyperparameters for latent space, for each of the 6 best autoencoder models and for each algorithm (brute force, lsh, cube, gnns, mrng, nsg) we evaluate the AAF between initial and latent space and the time needed for the search in latent space.

# 6. Clustering in Latent Space

## 6.1. Implementation - Evaluation

For clustering in latent space, we use the same approach as in nearest neighbor search in latent space. We encode whole dataset and query sets using the `encode` property of the model. Then, we save them to binary files as float32 in interval $[0,1]$ and load them back to C++. All points inside clusters can be projected back to initial space just by using their index in the initial dataset. However, this is not the case for the centroids of the clusters as it is not guaranteed that they belong to the dataset. For this reason, we need to use the `decode` property of the model to project them back to initial space where they can also not belong to the initial dataset. The decoding process is done in memory using ctypes.

For the evaluation, we use both silhouette metric and objective function value. For the objective function we find the below equation:

$$
	\sum_{i=1}^{k} \sum_{x \in S_i} \left\| x - c_i \right\|^2
$$

where $k$ is the number of clusters, $S_i$ is the $i$-th cluster and $c_i$ is the centroid of the $i$-th cluster. The value of the objective function gives a local minimum for the clustering problem generated from kmeans algorithms.

## 6.2 Results

Analysis is done in 2 notebooks:

- `grid_search_kmeans_latentnonproj_and_init.ipynb`: using the best hyperparameters for both latent and initial space, we calculate difference in clustering time and metrics. Here, comparing the metrics does not make sense as we calculate them without projection to initial space, but it gives us an indication of the performance of the algorithms in lower dimensions.
- `grid_search_kmeans_lat_init.ipynb`: using the best hyperparameters for latent space, for each of the 6 best autoencoder models and for each algorithm (classic, reverse lsh, reverse cube) we evaluate the silhouette metric between initial and latent space and the objective function value.

# References

[1] LeCun, Y., Cortes, C., & Burges, C.. THE MNIST DATABASE
of handwritten digits. https://yann.lecun.com/exdb/mnist/

[2] k-means clustering - Wikipedia
https://en.wikipedia.org/wiki/K-means_clustering

[3] Building Autoencoders in Keras
https://blog.keras.io/building-autoencoders-in-keras.html

[4] Keras: The high-level API for TensorFlow
https://www.tensorflow.org/guide/keras

[5] ctypes — A foreign function library for Python
https://docs.python.org/3/library/ctypes.html

[6] Optuna - A hyperparameter optimization framework
https://optuna.org/