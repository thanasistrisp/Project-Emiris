{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import ctypes\n",
    "from ctypes import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "\n",
    "from helper_funcs import *\n",
    "\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "from params import get_aaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir('./models/')\n",
    "\n",
    "dataset = b'MNIST/input.dat'\n",
    "query   = b'MNIST/query.dat'\n",
    "\n",
    "model_to_files = {}\n",
    "for i, model in enumerate(models):\n",
    "    normalized_dataset = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_dataset.dat'\n",
    "    normalized_query   = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_query.dat'\n",
    "    encoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_dataset.dat'\n",
    "    encoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_query.dat'\n",
    "    decoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_dataset.dat'\n",
    "    decoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_query.dat'\n",
    "\n",
    "    model_to_files.update({models[i] : [normalized_dataset, normalized_query,\n",
    "                                        encoded_dataset, encoded_query,\n",
    "                                        decoded_dataset, decoded_query]})\n",
    "\n",
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_to_files:\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    model = b'models/' + model.encode()\n",
    "\n",
    "    # load model\n",
    "    autoencoder = load_model(model.decode())\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # get shape of encoded layer\n",
    "\n",
    "    # load dataset\n",
    "    x_train = load_dataset(dataset)\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = load_dataset(query)\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    if len(shape) == 3: # if model type is convolutional\n",
    "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "    else:\n",
    "        x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "    encoded_train = autoencoder.encode(x_train)\n",
    "    encoded_test = autoencoder.encode(x_test)\n",
    "\n",
    "    # deflatten encoded datasets\n",
    "    encoded_train = deflatten_encoded(encoded_train, shape)\n",
    "    encoded_test = deflatten_encoded(encoded_test, shape)\n",
    "\n",
    "    # decode encoded datasets\n",
    "    decoded_train = autoencoder.decode(encoded_train)\n",
    "    decoded_test = autoencoder.decode(encoded_test)\n",
    "\n",
    "    # save original datasets normalized\n",
    "    save_decoded_binary(x_train, normalized_dataset)\n",
    "    save_decoded_binary(x_test, normalized_query)\n",
    "\n",
    "    # normalize encoded datasets\n",
    "    encoded_train = normalize(encoded_train)\n",
    "    encoded_test = normalize(encoded_test)\n",
    "\n",
    "    # save encoded datasets\n",
    "    save_encoded_binary(encoded_train, encoded_dataset)\n",
    "    save_encoded_binary(encoded_test, encoded_query)\n",
    "\n",
    "    # normalize decoded datasets\n",
    "    decoded_train = normalize(decoded_train)\n",
    "    decoded_test = normalize(decoded_test)\n",
    "\n",
    "    # save decoded datasets\n",
    "    save_decoded_binary(decoded_train, decoded_dataset)\n",
    "    save_decoded_binary(decoded_test, decoded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lsh = [7, 4, 1875, 0, 0.6]      # k, L, table_size, query_trick, window\n",
    "best_params_hypercube = [3, 67, 1000, 0.42] # k, M, probes, window\n",
    "\n",
    "# k, E, R\n",
    "best_params_gnns = {'model_conv_46.keras':  [92, 44, 9],\n",
    "                    'model_conv_12.keras':  [86, 59, 8],\n",
    "                    'model_conv_19.keras':  [96, 87, 7],\n",
    "                    'model_dense_26.keras': [70, 69, 9],\n",
    "                    'model_dense_43.keras': [100, 66, 10], \n",
    "                    'model_dense_1.keras':  [64, 58, 10]}\n",
    "\n",
    "# l\n",
    "best_params_mrng = {'model_dense_26.keras': [883],\n",
    "                    'model_conv_46.keras':  [859],\n",
    "                    'model_conv_19.keras':  [507],\n",
    "                    'model_dense_1.keras':  [428],\n",
    "                    'model_dense_43.keras': [818], \n",
    "                    'model_conv_12.keras':  [814]}\n",
    "\n",
    "# l, m, k, lq\n",
    "best_params_nsg = {'model_dense_26.keras': [987, 52, 69, 667],\n",
    "                   'model_conv_46.keras':  [987, 43, 87, 476],\n",
    "                   'model_dense_43.keras': [960, 102, 86, 768],\n",
    "                   'model_dense_1.keras':  [996, 11, 98, 657],\n",
    "                   'model_conv_12.keras':  [635, 197, 50, 907],\n",
    "                   'model_conv_19.keras':  [663, 99, 41, 533]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_brute_force(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    conf = {\n",
    "        'model': b'BRUTE',\n",
    "        'vals': [],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    aaf_lat_init, average_time = get_aaf(100, conf)\n",
    "\n",
    "    print(\"Average time           :\", average_time.value)\n",
    "    print(\"AAF (latent to initial):\", aaf_lat_init.value)\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    rows.append([model, 'BRUTE', average_time.value, aaf_lat_init.value])\n",
    "\n",
    "def run_lsh(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "    \n",
    "    conf = {\n",
    "        'model': b'LSH',\n",
    "        'vals': best_params_lsh[:-1],\n",
    "        'window': best_params_lsh[-1],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    aaf_lat_init, average_time = get_aaf(100, conf)\n",
    "\n",
    "    print(\"Average time           :\", average_time.value)\n",
    "    print(\"AAF (latent to initial):\", aaf_lat_init.value)\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    rows.append([model, 'LSH', average_time.value, aaf_lat_init.value])\n",
    "\n",
    "def run_hypercube(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "    \n",
    "    conf = {\n",
    "        'model': b'CUBE',\n",
    "        'vals': best_params_lsh[:-1],\n",
    "        'window': best_params_lsh[-1],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    aaf_lat_init, average_time = get_aaf(100, conf)\n",
    "\n",
    "    print(\"Average time           :\", average_time.value)\n",
    "    print(\"AAF (latent to initial):\", aaf_lat_init.value)\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    rows.append([model, 'CUBE', average_time.value, aaf_lat_init.value])\n",
    "\n",
    "def run_gnns(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "    \n",
    "    conf = {\n",
    "        'model': b'GNNS',\n",
    "        'vals': best_params_gnns[model],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    aaf_lat_init, average_time = get_aaf(100, conf)\n",
    "\n",
    "    print(\"Average time           :\", average_time.value)\n",
    "    print(\"AAF (latent to initial):\", aaf_lat_init.value)\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    rows.append([model, 'GNNS', average_time.value, aaf_lat_init.value])\n",
    "\n",
    "def run_mrng(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "    \n",
    "    conf = {\n",
    "        'model': b'MRNG',\n",
    "        'vals': best_params_mrng[model],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    aaf_lat_init, average_time = get_aaf(100, conf)\n",
    "\n",
    "    print(\"Average time           :\", average_time.value)\n",
    "    print(\"AAF (latent to initial):\", aaf_lat_init.value)\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    rows.append([model, 'MRNG', average_time.value, aaf_lat_init.value])\n",
    "\n",
    "def run_nsg(model):\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "    \n",
    "    conf = {\n",
    "        'model': b'NSG',\n",
    "        'vals': best_params_nsg[model],\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    aaf_lat_init, average_time = get_aaf(100, conf)\n",
    "\n",
    "    print(\"Average time           :\", average_time.value)\n",
    "    print(\"AAF (latent to initial):\", aaf_lat_init.value)\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    rows.append([model, 'NSG', average_time.value, aaf_lat_init.value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms_to_functions = {'BRUTE': run_brute_force,\n",
    "                           'LSH'  : run_lsh,\n",
    "                           'CUBE' : run_hypercube,\n",
    "                           'GNNS' : run_gnns,\n",
    "                           'MRNG' : run_mrng,\n",
    "                           'NSG'  : run_nsg}\n",
    "\n",
    "pool = ThreadPool(processes=4)\n",
    "\n",
    "rows = []\n",
    "for model in models:\n",
    "    for algorithm in algorithms_to_functions:\n",
    "        pool.apply_async(algorithms_to_functions[algorithm], (model,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_models, col_algorithms, col_average_time, col_aaf_lat_init = [], [], [], []\n",
    "\n",
    "for row in rows:\n",
    "    model, algorithm, average_time, aaf_lat_init = row\n",
    "\n",
    "    col_models.append(model)\n",
    "    col_algorithms.append(algorithm)\n",
    "    col_average_time.append(average_time)\n",
    "    col_aaf_lat_init.append(aaf_lat_init)\n",
    "\n",
    "col_dict = {'model': col_models, 'algorithm': col_algorithms,\n",
    "            'average time': col_average_time, 'AAF (latent to initial)': col_aaf_lat_init}\n",
    "\n",
    "df = pandas.DataFrame(data=col_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.sort_values(by='model', ascending=True)\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.sort_values(by='algorithm', ascending=True)\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
