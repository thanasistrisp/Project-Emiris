{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of results in initial and latent space\n",
    "\n",
    "[1. Optimize LSH](#1-optimize-lsh)\n",
    "\n",
    "[2. Optimize Hypercube](#2-optimize-hypercube)\n",
    "\n",
    "[3. Optimize K-Means](#3-optimize-k-means)\n",
    "\n",
    "[4. Optimize GNNS](#4-optimize-gnns)\n",
    "\n",
    "[5. Optimize MRNG](#5-optimize-mrng)\n",
    "\n",
    "[6. Optimize NSG](#6-optimize-nsg)\n",
    "\n",
    "[7. Grid Search](#7-grid-search)\n",
    "\n",
    "[8. Results](#8-results)\n",
    "\n",
    "+ [8. a. Optimization Results](#8-a-optimization-results)\n",
    "\n",
    "+ [8. b. Grid Search Results](#8-b-grid-search-results)\n",
    "\n",
    "[9. Conclusions](#9-conclusions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "from helper_funcs import *\n",
    "\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_pareto_front, plot_optimization_history, plot_slice\n",
    "\n",
    "from params import lsh_test, hypercube_test, kmeans_test, gnn_test, mrng_test, nsg_test, get_aaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir('./models/')\n",
    "\n",
    "dataset = b'MNIST/input.dat'\n",
    "query   = b'MNIST/query.dat'\n",
    "\n",
    "model_to_files = {}\n",
    "for i, model in enumerate(models):\n",
    "    normalized_dataset = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_dataset.dat'\n",
    "    normalized_query   = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_query.dat'\n",
    "    encoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_dataset.dat'\n",
    "    encoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_query.dat'\n",
    "    decoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_dataset.dat'\n",
    "    decoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_query.dat'\n",
    "\n",
    "    model_to_files.update({models[i] : [normalized_dataset, normalized_query,\n",
    "                                        encoded_dataset, encoded_query,\n",
    "                                        decoded_dataset, decoded_query]})\n",
    "\n",
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_to_files:\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    model = b'models/' + model.encode()\n",
    "\n",
    "    # load model\n",
    "    autoencoder = load_model(model.decode())\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # get shape of encoded layer\n",
    "\n",
    "    # load dataset\n",
    "    x_train = load_dataset(dataset)\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = load_dataset(query)\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    if len(shape) == 3: # if model type is convolutional\n",
    "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "    else:\n",
    "        x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "    encoded_train = autoencoder.encode(x_train)\n",
    "    encoded_test = autoencoder.encode(x_test)\n",
    "\n",
    "    # deflatten encoded datasets\n",
    "    encoded_train = deflatten_encoded(encoded_train, shape)\n",
    "    encoded_test = deflatten_encoded(encoded_test, shape)\n",
    "\n",
    "    # decode encoded datasets\n",
    "    decoded_train = autoencoder.decode(encoded_train)\n",
    "    decoded_test = autoencoder.decode(encoded_test)\n",
    "\n",
    "    # save original datasets normalized\n",
    "    save_decoded_binary(x_train, normalized_dataset)\n",
    "    save_decoded_binary(x_test, normalized_query)\n",
    "\n",
    "    # normalize encoded datasets\n",
    "    encoded_train = normalize(encoded_train)\n",
    "    encoded_test = normalize(encoded_test)\n",
    "\n",
    "    # save encoded datasets\n",
    "    save_encoded_binary(encoded_train, encoded_dataset)\n",
    "    save_encoded_binary(encoded_test, encoded_query)\n",
    "\n",
    "    # normalize decoded datasets\n",
    "    decoded_train = normalize(decoded_train)\n",
    "    decoded_test = normalize(decoded_test)\n",
    "\n",
    "    # save decoded datasets\n",
    "    save_decoded_binary(decoded_train, decoded_dataset)\n",
    "    save_decoded_binary(decoded_test, decoded_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Optimize LSH\n",
    "\n",
    "To skip logs, click [here](#visualize-lsh-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be copied from optimize_lsh.ipynb (3 cells)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize LSH study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be copied from optimize_lsh.ipynb (5 cells)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimize Hypercube\n",
    "\n",
    "## Optimize probes\n",
    "\n",
    "To skip logs, click [here](#visualize-hypercube-study-results-probes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_hypercube(trial):\n",
    "    model = trial.suggest_categorical('model', model_to_files.keys())\n",
    "    param_dict = {'k': trial.suggest_int('k', 2, 30),\n",
    "                  'probes': trial.suggest_int('probes', 1, 1000),\n",
    "                  'N': trial.suggest_int('N', 1, 10),\n",
    "                  'window': trial.suggest_float('window_size', 0.01, 1)}\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    encoded_dataset, encoded_query = model_to_files[model][2:4]\n",
    "    \n",
    "    average_time, aaf_latent = hypercube_test(encoded_dataset, encoded_query, queries_num=100, **param_dict, M = 60000, int_data=0)\n",
    "\n",
    "    return aaf_latent.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hypercube_study = optuna.create_study(study_name='hypercube', directions=['minimize', 'minimize'])\n",
    "hypercube_study.optimize(objective_hypercube, n_trials=50, n_jobs=-1)\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "trials = sorted(hypercube_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hypercube_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Hypercube study results (probes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(hypercube_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(hypercube_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(hypercube_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(hypercube_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(hypercube_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize M\n",
    "\n",
    "To skip logs, click [here](#visualize-hypercube-study-results-m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_hypercube(trial):\n",
    "    model = trial.suggest_categorical('model', model_to_files.keys())\n",
    "    param_dict = {'k': trial.suggest_int('k', 2, 30),\n",
    "                  'M': trial.suggest_int('M', 10, 1000),\n",
    "                  'N': trial.suggest_int('N', 1, 10)}\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    encoded_dataset, encoded_query = model_to_files[model][2:4]\n",
    "    \n",
    "    average_time, aaf_latent = hypercube_test(encoded_dataset, encoded_query, queries_num=100, **param_dict, probes = 5000)\n",
    "\n",
    "    return aaf_latent.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hypercube_study = optuna.create_study(study_name='hypercube', directions=['minimize', 'minimize'])\n",
    "hypercube_study.optimize(objective_hypercube, n_trials=50, n_jobs=-1)\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "trials = sorted(hypercube_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hypercube_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Hypercube study results (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(hypercube_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(hypercube_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(hypercube_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(hypercube_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(hypercube_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimize K-Means\n",
    "\n",
    "To skip logs, click [here](#visualize-k-means-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_kmeans(trial):\n",
    "    model = trial.suggest_categorical('model', model_to_files.keys())\n",
    "    param_dict = {'method': trial.suggest_categorical('method', ['CLASSIC', 'LSH', 'CUBE'])}\n",
    "    \n",
    "    enc_vals = []\n",
    "    if param_dict['method'] == 'LSH':\n",
    "        param_dict.update({'k': trial.suggest_int('k', 1, 10)})\n",
    "        param_dict.update({'L': trial.suggest_int('L', 1, 10)})\n",
    "        param_dict.update({'window': trial.suggest_float('window', 0.01, 1)})\n",
    "        enc_vals = [param_dict['k'], param_dict['L']]\n",
    "    elif param_dict['method'] == 'CUBE':\n",
    "        param_dict.update({'M': trial.suggest_int('M', 10, 5000)})\n",
    "        param_dict.update({'k': trial.suggest_int('k', 2, 30)})\n",
    "        param_dict.update({'probes': trial.suggest_int('probes', 1, 1000)})\n",
    "        param_dict.update({'window': trial.suggest_float('window', 0.01, 1)})\n",
    "        enc_vals = [param_dict['M'], param_dict['k'], param_dict['probes']]\n",
    "\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    config = {\n",
    "        'model': bytes(param_dict['method'], encoding='ascii'),\n",
    "        'enc_vals': enc_vals,\n",
    "        'dataset': normalized_dataset,\n",
    "        'query': normalized_query,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "        'decoded_dataset': decoded_dataset,\n",
    "    }\n",
    "\n",
    "    if param_dict['method'] != 'CLASSIC':\n",
    "        config.update({'window': param_dict['window']})\n",
    "\n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    average_time, stotal_latent, silhouette = kmeans_test(conf=config, int_data=0)\n",
    "\n",
    "    print(\"Silhouette per cluster:\", silhouette.val)\n",
    "\n",
    "    del silhouette\n",
    "\n",
    "    return stotal_latent.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans_study = optuna.create_study(study_name='kmeans', directions=['minimize', 'minimize'])\n",
    "kmeans_study.optimize(objective_kmeans, n_trials=50, n_jobs=-1)\n",
    "print(\"-------------------- Best trials --------------------\")\n",
    "trials = sorted(kmeans_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kmeans_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize K-Means study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(kmeans_study, target_names=['stotal', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(kmeans_study, target = lambda t: t.values[0], target_name = 'stotal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(kmeans_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(kmeans_study, target = lambda t: t.values[0], target_name = 'stotal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(kmeans_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimize GNNS\n",
    "\n",
    "To skip logs, click [here](#visualize-gnns-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_gnns(trial):\n",
    "    model = trial.suggest_categorical('model', model_to_files.keys())\n",
    "    param_dict = {'k': trial.suggest_int('k', 40, 100)}\n",
    "    param_dict.update({'E': trial.suggest_int('E', 40, param_dict['k'])})\n",
    "    param_dict.update({'R': trial.suggest_int('R', 1, 10)})\n",
    "\n",
    "    print(\"Trial params\", param_dict)\n",
    "\n",
    "    encoded_dataset, encoded_query = model_to_files[model][2:4]\n",
    "\n",
    "    average_time, aaf_latent = gnn_test(encoded_dataset, encoded_query, queries_num=100, **param_dict, N=5, int_data=0)\n",
    "\n",
    "    return aaf_latent.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gnns_study = optuna.create_study(study_name='gnns', directions=['minimize', 'minimize'])\n",
    "gnns_study.optimize(objective_gnns, n_trials=100, n_jobs=-1)\n",
    "print(\"-------------------- Best trials --------------------\")\n",
    "trials = sorted(gnns_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gnns_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize GNNS study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(gnns_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(gnns_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(gnns_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(gnns_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(gnns_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimize MRNG\n",
    "\n",
    "To skip logs, click [here](#visualize-mrng-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mrng(trial):\n",
    "    model = trial.suggest_categorical('model', model_to_files.keys())\n",
    "    param_dict = {'l': trial.suggest_int('l', 1, 1000)}\n",
    "    param_dict.update({'N': trial.suggest_int('N', 1, param_dict['l'])})\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    encoded_dataset, encoded_query = model_to_files[model][2:4]\n",
    "\n",
    "    average_time, aaf_latent = mrng_test(encoded_dataset, encoded_query, queries_num=100, **param_dict, int_data=0)\n",
    "\n",
    "    return aaf_latent.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mrng_study = optuna.create_study(study_name='mrng', directions=['minimize', 'minimize'])\n",
    "mrng_study.optimize(objective_mrng, n_trials=50, n_jobs=-1)\n",
    "print(\"-------------------- Best trials --------------------\")\n",
    "trials = sorted(mrng_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mrng_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize MRNG study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(mrng_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(mrng_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(mrng_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(mrng_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(mrng_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Optimize NSG\n",
    "\n",
    "To skip logs, click [here](#visualize-nsg-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_nsg(trial):\n",
    "    model = trial.suggest_categorical('model', model_to_files.keys())\n",
    "    param_dict = {'m' : trial.suggest_int('m', 3, 200),\n",
    "                  'l' : trial.suggest_int('l', 10, 1000),\n",
    "                  'lq': trial.suggest_int('lq', 1, 1000),\n",
    "                  'k' : trial.suggest_int('k', 40, 100)}\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    encoded_dataset, encoded_query = model_to_files[model][2:4]\n",
    "\n",
    "    average_time, aaf_latent = nsg_test(encoded_dataset, encoded_query, queries_num=100, **param_dict, N=5, int_data=0)\n",
    "\n",
    "    return aaf_latent.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nsg_study = optuna.create_study(study_name='nsg', directions=['minimize', 'minimize'])\n",
    "nsg_study.optimize(objective_nsg, n_trials=100, n_jobs=-1)\n",
    "print(\"-------------------- Best trials --------------------\")\n",
    "trials = sorted(nsg_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nsg_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize NSG study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(nsg_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(nsg_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(nsg_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(nsg_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(nsg_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Grid Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Results\n",
    "\n",
    "## 8. a. Optimization Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. b. Grid Search Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
