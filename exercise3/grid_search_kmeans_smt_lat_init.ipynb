{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import ctypes\n",
    "from ctypes import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "\n",
    "from helper_funcs import *\n",
    "\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "from params import get_kmeans_eval_object, get_centroids, convert_to_2d_array, get_stotal, free_centroids, free_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir('./models/')\n",
    "\n",
    "dataset = b'MNIST/input.dat'\n",
    "query   = b'MNIST/query.dat'\n",
    "\n",
    "model_to_files = {}\n",
    "for i, model in enumerate(models):\n",
    "    normalized_dataset = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_dataset.dat'\n",
    "    normalized_query   = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_query.dat'\n",
    "    encoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_dataset.dat'\n",
    "    encoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_query.dat'\n",
    "    decoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_dataset.dat'\n",
    "    decoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_decoded_query.dat'\n",
    "\n",
    "    model_to_files.update({models[i] : [normalized_dataset, normalized_query,\n",
    "                                        encoded_dataset, encoded_query,\n",
    "                                        decoded_dataset, decoded_query]})\n",
    "\n",
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_to_files:\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    model = b'models/' + model.encode()\n",
    "\n",
    "    # load model\n",
    "    autoencoder = load_model(model.decode())\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # get shape of encoded layer\n",
    "\n",
    "    # load dataset\n",
    "    x_train = load_dataset(dataset)\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = load_dataset(query)\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    if len(shape) == 3: # if model type is convolutional\n",
    "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "    else:\n",
    "        x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "    encoded_train = autoencoder.encode(x_train)\n",
    "    encoded_test = autoencoder.encode(x_test)\n",
    "\n",
    "    # deflatten encoded datasets\n",
    "    encoded_train = deflatten_encoded(encoded_train, shape)\n",
    "    encoded_test = deflatten_encoded(encoded_test, shape)\n",
    "\n",
    "    # decode encoded datasets\n",
    "    decoded_train = autoencoder.decode(encoded_train)\n",
    "    decoded_test = autoencoder.decode(encoded_test)\n",
    "\n",
    "    # save original datasets normalized\n",
    "    save_decoded_binary(x_train, normalized_dataset)\n",
    "    save_decoded_binary(x_test, normalized_query)\n",
    "\n",
    "    # normalize encoded datasets\n",
    "    encoded_train = normalize(encoded_train)\n",
    "    encoded_test = normalize(encoded_test)\n",
    "\n",
    "    # save encoded datasets\n",
    "    save_encoded_binary(encoded_train, encoded_dataset)\n",
    "    save_encoded_binary(encoded_test, encoded_query)\n",
    "\n",
    "    # normalize decoded datasets\n",
    "    decoded_train = normalize(decoded_train)\n",
    "    decoded_test = normalize(decoded_test)\n",
    "\n",
    "    # save decoded datasets\n",
    "    save_decoded_binary(decoded_train, decoded_dataset)\n",
    "    save_decoded_binary(decoded_test, decoded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lsh = [4, 7, 1, 0.6]            # L, k, limit_queries, window\n",
    "best_params_hypercube = [67, 3, 1000, 0.42] # M, k, probes, window\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(stotal_lat_init, silouette_lat_init):\n",
    "    print(\"Total silhouette:\", stotal_lat_init)\n",
    "    print(\"Silhouette per cluster:\", silouette_lat_init)\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "def convert_centroids_to_2d_array(model, centroids):\n",
    "    model = b'models/' + model.encode()\n",
    "\n",
    "    autoencoder = load_model(model.decode())\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # shape of encoded layer\n",
    "\n",
    "    centroids = deflatten_encoded(centroids, shape)\n",
    "    decoded_centroids = autoencoder.decode(centroids)\n",
    "    decoded_centroids = flatten_encoded(decoded_centroids)\n",
    "\n",
    "    decoded_centroids = decoded_centroids.astype(np.float64)\n",
    "    decoded_centroids = decoded_centroids.flatten()\n",
    "    decoded_centroids = decoded_centroids.ctypes.data_as(ctypes.POINTER(ctypes.c_double))\n",
    "    decoded_centroids = convert_to_2d_array(decoded_centroids, 784)\n",
    "\n",
    "    return decoded_centroids\n",
    "\n",
    "def run_kmeans_classic(model):\n",
    "    normalized_dataset, encoded_dataset = model_to_files[model][0], model_to_files[model][2]\n",
    "\n",
    "    config = {\n",
    "        'model': b'CLASSIC',\n",
    "        'vals': [],\n",
    "        'dataset': normalized_dataset,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "    }\n",
    "    \n",
    "    kmeans = get_kmeans_eval_object(conf=config)\n",
    "    centroids, dim = get_centroids(kmeans)\n",
    "\n",
    "    decoded_centroids = convert_centroids_to_2d_array(model, centroids)\n",
    "\n",
    "    # total silhouette and silhouette per cluster in latent space converted to initial space\n",
    "    stotal_lat_init, silouette_lat_init = get_stotal(config, dim, kmeans, decoded_centroids)\n",
    "\n",
    "    free_centroids(decoded_centroids)\n",
    "    free_kmeans(kmeans)\n",
    "\n",
    "    print_results(stotal_lat_init.value, silouette_lat_init.val)\n",
    "\n",
    "    rows.append([model, 'CLASSIC', stotal_lat_init.value])\n",
    "\n",
    "    del silouette_lat_init\n",
    "\n",
    "def run_kmeans_lsh(model):\n",
    "    normalized_dataset, encoded_dataset = model_to_files[model][0], model_to_files[model][2]\n",
    "\n",
    "    config = {\n",
    "        'model': b'LSH',\n",
    "        'vals': best_params_lsh[:-1],\n",
    "        'window': best_params_lsh[-1],\n",
    "        'dataset': normalized_dataset,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "    }\n",
    "\n",
    "    kmeans = get_kmeans_eval_object(conf=config)\n",
    "    centroids, dim = get_centroids(kmeans)\n",
    "\n",
    "    decoded_centroids = convert_centroids_to_2d_array(model, centroids)\n",
    "\n",
    "    # total silhouette and silhouette per cluster in latent space converted to initial space\n",
    "    stotal_lat_init, silouette_lat_init = get_stotal(config, dim, kmeans, decoded_centroids)\n",
    "\n",
    "    free_centroids(decoded_centroids)\n",
    "    free_kmeans(kmeans)\n",
    "\n",
    "    print_results(stotal_lat_init.value, silouette_lat_init.val)\n",
    "\n",
    "    rows.append([model, 'LSH', stotal_lat_init.value])\n",
    "\n",
    "    del silouette_lat_init\n",
    "\n",
    "def run_kmeans_cube(model):\n",
    "    normalized_dataset, encoded_dataset = model_to_files[model][0], model_to_files[model][2]\n",
    "\n",
    "    config = {\n",
    "        'model': b'CUBE',\n",
    "        'vals': best_params_hypercube[:-1],\n",
    "        'window': best_params_hypercube[-1],\n",
    "        'dataset': normalized_dataset,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "    }\n",
    "\n",
    "    kmeans = get_kmeans_eval_object(conf=config)\n",
    "    centroids, dim = get_centroids(kmeans)\n",
    "\n",
    "    decoded_centroids = convert_centroids_to_2d_array(model, centroids)\n",
    "\n",
    "    # total silhouette and silhouette per cluster in latent space converted to initial space\n",
    "    stotal_lat_init, silouette_lat_init = get_stotal(config, dim, kmeans, decoded_centroids)\n",
    "\n",
    "    free_centroids(decoded_centroids)\n",
    "    free_kmeans(kmeans)\n",
    "\n",
    "    print_results(stotal_lat_init.value, silouette_lat_init.val)\n",
    "\n",
    "    rows.append([model, 'CUBE', stotal_lat_init.value])\n",
    "\n",
    "    del silouette_lat_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = ThreadPool(processes=4)\n",
    "\n",
    "for model in models:\n",
    "    pool.apply_async(run_kmeans_classic, (model,))\n",
    "    pool.apply_async(run_kmeans_lsh, (model,))\n",
    "    pool.apply_async(run_kmeans_cube, (model,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_models, col_methods, col_stotal_lat_init = [], [], []\n",
    "\n",
    "for row in rows:\n",
    "    model, method, stotal_lat_init = row\n",
    "\n",
    "    col_models.append(model)\n",
    "    col_methods.append(method)\n",
    "    col_stotal_lat_init.append(stotal_lat_init)\n",
    "\n",
    "col_dict = {'model': col_models, 'method': col_methods, 'silhouette total (latent to initial)': col_stotal_lat_init}\n",
    "\n",
    "df = pandas.DataFrame(data=col_dict)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
