{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for K-Means in Latent Space (+ Comparison to Initial Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import ctypes\n",
    "from ctypes import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "\n",
    "from helper_funcs import *\n",
    "\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', None)\n",
    "\n",
    "from params import get_kmeans_eval_object, get_centroids, convert_to_2d_array, get_stotal, free_centroids, free_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T10:11:52.045761Z",
     "iopub.status.busy": "2024-01-04T10:11:52.045064Z",
     "iopub.status.idle": "2024-01-04T10:11:52.051374Z",
     "shell.execute_reply": "2024-01-04T10:11:52.050768Z"
    }
   },
   "outputs": [],
   "source": [
    "models = os.listdir('./models/')\n",
    "\n",
    "dataset = b'MNIST/input.dat'\n",
    "query   = b'MNIST/query.dat'\n",
    "\n",
    "model_to_files = {}\n",
    "for i, model in enumerate(models):\n",
    "    normalized_dataset = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_dataset.dat'\n",
    "    normalized_query   = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_normalized_query.dat'\n",
    "    encoded_dataset    = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_dataset.dat'\n",
    "    encoded_query      = b'MNIST/' + models[i].removesuffix('.keras').encode() + b'_encoded_query.dat'\n",
    "\n",
    "    model_to_files.update({models[i] : [normalized_dataset, normalized_query,\n",
    "                                        encoded_dataset, encoded_query]})\n",
    "\n",
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T10:11:52.056344Z",
     "iopub.status.busy": "2024-01-04T10:11:52.055883Z",
     "iopub.status.idle": "2024-01-04T10:15:04.247189Z",
     "shell.execute_reply": "2024-01-04T10:15:04.246244Z"
    }
   },
   "outputs": [],
   "source": [
    "for model in model_to_files:\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query = model_to_files[model]\n",
    "\n",
    "    model = b'models/' + model.encode()\n",
    "\n",
    "    # load model\n",
    "    autoencoder = load_model(model.decode())\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # get shape of encoded layer\n",
    "\n",
    "    # load dataset\n",
    "    x_train = load_dataset(dataset)\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = load_dataset(query)\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    if len(shape) == 3: # if model type is convolutional\n",
    "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "    else:\n",
    "        x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "    encoded_train = autoencoder.encode(x_train)\n",
    "    encoded_test = autoencoder.encode(x_test)\n",
    "\n",
    "    # deflatten encoded datasets\n",
    "    encoded_train = deflatten_encoded(encoded_train, shape)\n",
    "    encoded_test = deflatten_encoded(encoded_test, shape)\n",
    "\n",
    "    # save original datasets normalized\n",
    "    save_decoded_binary(x_train, normalized_dataset)\n",
    "    save_decoded_binary(x_test, normalized_query)\n",
    "\n",
    "    # normalize encoded datasets\n",
    "    encoded_train = normalize(encoded_train)\n",
    "    encoded_test = normalize(encoded_test)\n",
    "\n",
    "    # save encoded datasets\n",
    "    save_encoded_binary(encoded_train, encoded_dataset)\n",
    "    save_encoded_binary(encoded_test, encoded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T10:15:04.252692Z",
     "iopub.status.busy": "2024-01-04T10:15:04.251940Z",
     "iopub.status.idle": "2024-01-04T10:15:04.258715Z",
     "shell.execute_reply": "2024-01-04T10:15:04.257719Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params_lsh = [4, 7, 1, 0.6]            # L, k, limit_queries, window\n",
    "best_params_hypercube = [67, 3, 1000, 0.42] # M, k, probes, window\n",
    "\n",
    "rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T10:15:04.264175Z",
     "iopub.status.busy": "2024-01-04T10:15:04.263715Z",
     "iopub.status.idle": "2024-01-04T10:15:04.280804Z",
     "shell.execute_reply": "2024-01-04T10:15:04.279624Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_results(stotal_lat_init, silouette_lat_init, obj_func):\n",
    "    print(\"Total silhouette:\", stotal_lat_init)\n",
    "    print(\"Silhouette per cluster:\", silouette_lat_init)\n",
    "    print(\"Objective function:\", obj_func)\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "def decode_centroids(model, centroids): # decodes centroids and returns them as c array\n",
    "    model = b'models/' + model.encode()\n",
    "\n",
    "    autoencoder = load_model(model.decode())\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # shape of encoded layer\n",
    "\n",
    "    centroids = deflatten_encoded(centroids, shape)\n",
    "    decoded_centroids = autoencoder.decode(centroids)\n",
    "    decoded_centroids = flatten_encoded(decoded_centroids)\n",
    "\n",
    "    decoded_centroids = decoded_centroids.astype(np.float64) # cast to float64, equivalent to double in c\n",
    "    decoded_centroids = decoded_centroids.flatten() # flatten to 1d numpy array\n",
    "    decoded_centroids = decoded_centroids.ctypes.data_as(ctypes.POINTER(ctypes.c_double)) # convert to c array\n",
    "    decoded_centroids = convert_to_2d_array(decoded_centroids, 784) # convert to 2d c array, shape: (10, 784)\n",
    "\n",
    "    return decoded_centroids\n",
    "\n",
    "def compute_silhouette(model, config):\n",
    "    kmeans = get_kmeans_eval_object(conf=config)\n",
    "    centroids, dim = get_centroids(kmeans)\n",
    "\n",
    "    decoded_centroids = decode_centroids(model, centroids)\n",
    "\n",
    "    # total silhouette and silhouette per cluster in latent space converted to initial space\n",
    "    stotal_lat_init, silouette_lat_init, obj_val = get_stotal(config, dim, kmeans, decoded_centroids)\n",
    "\n",
    "    free_centroids(decoded_centroids)\n",
    "    free_kmeans(kmeans)\n",
    "\n",
    "    print_results(stotal_lat_init.value, silouette_lat_init.val, obj_val.value)\n",
    "\n",
    "    rows.append([model, config['model'].decode(), stotal_lat_init.value, obj_val.value])\n",
    "\n",
    "    del silouette_lat_init\n",
    "\n",
    "def run_kmeans_classic(model):\n",
    "    normalized_dataset, encoded_dataset = model_to_files[model][0], model_to_files[model][2]\n",
    "\n",
    "    config = {\n",
    "        'model': b'CLASSIC',\n",
    "        'vals': [],\n",
    "        'dataset': normalized_dataset,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "    }\n",
    "    \n",
    "    compute_silhouette(model, config)\n",
    "\n",
    "def run_kmeans_lsh(model):\n",
    "    normalized_dataset, encoded_dataset = model_to_files[model][0], model_to_files[model][2]\n",
    "\n",
    "    config = {\n",
    "        'model': b'LSH',\n",
    "        'vals': best_params_lsh[:-1],\n",
    "        'window': best_params_lsh[-1],\n",
    "        'dataset': normalized_dataset,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "    }\n",
    "\n",
    "    compute_silhouette(model, config)\n",
    "\n",
    "def run_kmeans_cube(model):\n",
    "    normalized_dataset, encoded_dataset = model_to_files[model][0], model_to_files[model][2]\n",
    "\n",
    "    config = {\n",
    "        'model': b'CUBE',\n",
    "        'vals': best_params_hypercube[:-1],\n",
    "        'window': best_params_hypercube[-1],\n",
    "        'dataset': normalized_dataset,\n",
    "        'encoded_dataset': encoded_dataset,\n",
    "    }\n",
    "\n",
    "    compute_silhouette(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs 4 threads in parallel at a time. When one finishes, next one is started. There is no need for locking as Python's code is always executed in a single thread because of GIL (Global Interpreter Lock). C++ code is executed in its own thread (and CPU for our case) and is not affected by GIL as it is released from ctypes module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T10:15:04.285229Z",
     "iopub.status.busy": "2024-01-04T10:15:04.284837Z",
     "iopub.status.idle": "2024-01-04T12:04:41.059695Z",
     "shell.execute_reply": "2024-01-04T12:04:41.058984Z"
    }
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(processes=4)\n",
    "\n",
    "for model in models:\n",
    "    pool.apply_async(run_kmeans_classic, (model,))\n",
    "    pool.apply_async(run_kmeans_lsh, (model,))\n",
    "    pool.apply_async(run_kmeans_cube, (model,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T12:04:41.063691Z",
     "iopub.status.busy": "2024-01-04T12:04:41.063381Z",
     "iopub.status.idle": "2024-01-04T12:04:41.081454Z",
     "shell.execute_reply": "2024-01-04T12:04:41.080583Z"
    }
   },
   "outputs": [],
   "source": [
    "col_models, col_methods, col_stotal_lat_init, col_obj_func = [], [], [], []\n",
    "\n",
    "for row in rows:\n",
    "    model, method, stotal_lat_init, obj_func = row\n",
    "\n",
    "    col_models.append(model)\n",
    "    col_methods.append(method)\n",
    "    col_stotal_lat_init.append(stotal_lat_init)\n",
    "    col_obj_func.append(obj_func)\n",
    "\n",
    "col_dict = {'model': col_models, 'method': col_methods, 'silhouette total (latent to initial)': col_stotal_lat_init, 'objective function': col_obj_func}\n",
    "\n",
    "df = pandas.DataFrame(data=col_dict)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
