{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from autoencoder import Autoencoder\n",
    "from helper_funcs import *\n",
    "\n",
    "import pandas\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_pareto_front, plot_optimization_history, plot_slice\n",
    "\n",
    "from params import lsh_test, hypercube_test, kmeans_test, gnn_test, mrng_test, nsg_test, get_aaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir('./models/')\n",
    "\n",
    "dataset = 'MNIST/input.dat'\n",
    "query = 'MNIST/query.dat'\n",
    "\n",
    "model_to_files = {}\n",
    "for i, model in enumerate(models):\n",
    "    normalized_dataset = 'MNIST/' + models[i].removesuffix('.keras') + '_normalized_dataset.dat'\n",
    "    normalized_query   = 'MNIST/' + models[i].removesuffix('.keras') + '_normalized_query.dat'\n",
    "    encoded_dataset    = 'MNIST/' + models[i].removesuffix('.keras') + '_encoded_dataset.dat'\n",
    "    encoded_query      = 'MNIST/' + models[i].removesuffix('.keras') + '_encoded_query.dat'\n",
    "    decoded_dataset    = 'MNIST/' + models[i].removesuffix('.keras') + '_decoded_dataset.dat'\n",
    "    decoded_query      = 'MNIST/' + models[i].removesuffix('.keras') + '_decoded_query.dat'\n",
    "\n",
    "    model_to_files.update({models[i] : [normalized_dataset, normalized_query,\n",
    "                                        encoded_dataset, encoded_query,\n",
    "                                        decoded_dataset, decoded_query]})\n",
    "\n",
    "n = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_to_files:\n",
    "    normalized_dataset, normalized_query, encoded_dataset, encoded_query, decoded_dataset, decoded_query = model_to_files[model]\n",
    "\n",
    "    model = 'models/' + model\n",
    "\n",
    "    # load model\n",
    "    autoencoder = load_model(model)\n",
    "    shape = autoencoder.layers[-2].output_shape[1:] # get shape of encoded layer\n",
    "\n",
    "    # load dataset\n",
    "    x_train = load_dataset(dataset)\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = load_dataset(query)\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    if len(shape) == 3: # if model type is convolutional\n",
    "        x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "    else:\n",
    "        x_train = np.reshape(x_train, (len(x_train), 784))\n",
    "        x_test = np.reshape(x_test, (len(x_test), 784))\n",
    "\n",
    "    encoded_train = autoencoder.encode(x_train)\n",
    "    encoded_test = autoencoder.encode(x_test)\n",
    "\n",
    "    # deflatten encoded datasets\n",
    "    encoded_train = deflatten_encoded(encoded_train, shape)\n",
    "    encoded_test = deflatten_encoded(encoded_test, shape)\n",
    "\n",
    "    # decode encoded datasets\n",
    "    decoded_train = autoencoder.decode(encoded_train)\n",
    "    decoded_test = autoencoder.decode(encoded_test)\n",
    "\n",
    "    # save original datasets normalized\n",
    "    save_decoded_binary(x_train, normalized_dataset)\n",
    "    save_decoded_binary(x_test, normalized_query)\n",
    "\n",
    "    # normalize encoded datasets\n",
    "    encoded_train = normalize(encoded_train)\n",
    "    encoded_test = normalize(encoded_test)\n",
    "\n",
    "    # save encoded datasets\n",
    "    save_encoded_binary(encoded_train, encoded_dataset)\n",
    "    save_encoded_binary(encoded_test, encoded_query)\n",
    "\n",
    "    # normalize decoded datasets\n",
    "    decoded_train = normalize(decoded_train)\n",
    "    decoded_test = normalize(decoded_test)\n",
    "\n",
    "    # save decoded datasets\n",
    "    save_decoded_binary(decoded_train, decoded_dataset)\n",
    "    save_decoded_binary(decoded_test, decoded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lsh(trial):\n",
    "    param_dict = {'model': trial.suggest_categorical('model', model_to_files.keys()),\n",
    "                  'k': trial.suggest_int('k', 1, 10),\n",
    "                  'L': trial.suggest_int('L', 1, 10),\n",
    "                  'table_size':  trial.suggest_categorical('table_size', [int(n/32), int(n/16), int(n/8)]),\n",
    "                  'window_size': trial.suggest_int('window_size', 0.01, 1),\n",
    "                  'query_trick': trial.suggest_categorical('query_trick', [True, False])\n",
    "                 }\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    encoded_dataset, encoded_query = model_to_files[param_dict['model']][2:4]\n",
    "\n",
    "    average_time, aaf_latent, min_neighbors = lsh_test(encoded_dataset, encoded_query, queries_num=100, **param_dict, N=60)\n",
    "\n",
    "    # trial should return at least 60 neighbors to be used in GNNS\n",
    "    # penalize model if slower than brute force\n",
    "    c0 = - min_neighbors.value + 60\n",
    "    c1 = average_time.value - 1e-4\n",
    "    trial.set_user_attr('constraint', (c0, c1))\n",
    "\n",
    "    return aaf_latent.value, average_time.value\n",
    "\n",
    "def constraints(trial):\n",
    "    return trial.user_attrs['constraint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampler = optuna.samplers.NSGAIISampler(constraints_func=constraints)\n",
    "lsh_study = optuna.create_study(study_name='lsh', directions=['minimize', 'minimize'], sampler=sampler)\n",
    "lsh_study.optimize(objective_lsh, n_trials=100, n_jobs=-1)\n",
    "print(\"-------------------- Best trials --------------------\")\n",
    "trials = sorted(lsh_study.best_trials, key=lambda x: x.values)\n",
    "# print feasible trials only\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}, Constraints = {}\".format(trial.values, trial.user_attrs[\"constraint\"]))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lsh_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['value'])\n",
    "df_sorted = df_sorted.sort_values(by=['value'], ascending=True)\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(lsh_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(lsh_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(lsh_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(lsh_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(lsh_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
