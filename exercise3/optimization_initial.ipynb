{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization - fine tuning in Initial Space\n",
    "\n",
    "[1. Optimize LSH](#1-optimize-lsh)\n",
    "\n",
    "[2. Optimize Hypercube](#2-optimize-hypercube)\n",
    "\n",
    "[3. Optimize GNNS](#3-optimize-gnns)\n",
    "\n",
    "[4. Optimize MRNG](#4-optimize-mrng)\n",
    "\n",
    "[5. Optimize NSG](#5-optimize-nsg)\n",
    "\n",
    "[6. Results](#6-results)\n",
    "\n",
    "[7. Conclusions](#7-conclusions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_pareto_front, plot_optimization_history, plot_slice\n",
    "\n",
    "from params import lsh_test, hypercube_test, gnn_test, mrng_test, nsg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = b'MNIST/input.dat'\n",
    "query_path = b'MNIST/query.dat'\n",
    "\n",
    "n = 60000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Optimize LSH\n",
    "\n",
    "To skip logs, click [here](#visualize-lsh-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lsh(trial):\n",
    "    param_dict = {'k': trial.suggest_int('k', 2, 10),\n",
    "                  'L': trial.suggest_int('L', 2, 10),\n",
    "                  'table_size':  trial.suggest_categorical('table_size', [int(n/16), int(n/8), int(n/4)]),\n",
    "                  'window_size': trial.suggest_int('window_size', 100, 5000),\n",
    "                  'query_trick': trial.suggest_categorical('query_trick', [True, False])\n",
    "                 }\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    average_time, aaf, min_neighbors = lsh_test(input_path, query_path, queries_num=100, **param_dict, N=60)\n",
    "\n",
    "    # trial should return at least 60 neighbors to be used in GNNS\n",
    "    # penalize model if slower than brute force\n",
    "    c0 = - min_neighbors.value + 60\n",
    "    c1 = average_time.value - 0.01\n",
    "    trial.set_user_attr('constraint', (c0, c1))\n",
    "\n",
    "    return aaf.value, average_time.value\n",
    "\n",
    "def constraints(trial):\n",
    "    return trial.user_attrs['constraint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    try:\n",
    "        sampler = optuna.samplers.NSGAIISampler(constraints_func=constraints)\n",
    "        lsh_study = optuna.create_study(study_name='lsh', directions=['minimize', 'minimize'], sampler=sampler)\n",
    "        lsh_study.optimize(objective_lsh, n_trials=50)\n",
    "        print(\"-------------------- Best trials --------------------\")\n",
    "        trials = sorted(lsh_study.best_trials, key=lambda x: x.values)\n",
    "        # print feasible trials only\n",
    "        for trial in trials:\n",
    "            print(\"Trial no. {}\".format(trial.number))\n",
    "            print(\" Values = {}, Constraints = {}\".format(trial.values, trial.user_attrs[\"constraint\"]))\n",
    "            print(\" Params = {}\".format(trial.params))\n",
    "        break\n",
    "    except:\n",
    "        print(\"Trial failed, trying again...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lsh_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize LSH study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(lsh_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(lsh_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(lsh_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(lsh_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(lsh_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimize Hypercube\n",
    "\n",
    "To skip logs, click [here](#visualize-hypercube-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_hypercube(trial):\n",
    "    param_dict = {'k': trial.suggest_int('k', 2, 30),\n",
    "                  'probes': trial.suggest_int('probes', 1, 1000),\n",
    "                  'N': trial.suggest_int('N', 1, 10)}\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "    \n",
    "    average_time, aaf = hypercube_test(input_path, query_path, queries_num=100, **param_dict, M = 60000)\n",
    "\n",
    "    return aaf.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hypercube_study = optuna.create_study(study_name='hypercube', directions=['minimize', 'minimize'])\n",
    "hypercube_study.optimize(objective_hypercube, n_trials=50)\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "trials = sorted(hypercube_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hypercube_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Hypercube study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(hypercube_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(hypercube_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(hypercube_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(hypercube_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(hypercube_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize M\n",
    "\n",
    "To skip logs, click [here](#visualize-hypercube-study-results-m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_hypercube(trial):\n",
    "    param_dict = {'k': trial.suggest_int('k', 2, 30),\n",
    "                  'M': trial.suggest_int('M', 10, 5000),\n",
    "                  'N': trial.suggest_int('N', 1, 10)}\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "    \n",
    "    average_time, aaf = hypercube_test(input_path, query_path, queries_num=100, **param_dict, probes = 5000)\n",
    "\n",
    "    return aaf.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hypercube_study = optuna.create_study(study_name='hypercube', directions=['minimize', 'minimize'])\n",
    "hypercube_study.optimize(objective_hypercube, n_trials=50)\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "trials = sorted(hypercube_study.best_trials, key=lambda x: x.values)\n",
    "for trial in trials:\n",
    "    print(\"Trial no. {}\".format(trial.number))\n",
    "    print(\" Values = {}\".format(trial.values))\n",
    "    print(\" Params = {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hypercube_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Hypercube study results (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(hypercube_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(hypercube_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(hypercube_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(hypercube_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(hypercube_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimize GNNS\n",
    "\n",
    "To skip logs, click [here](#visualize-gnns-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_gnns(trial):\n",
    "    param_dict = {'k': trial.suggest_int('k', 40, 100)}\n",
    "    param_dict.update({'E': trial.suggest_int('E', 40, param_dict['k'])})\n",
    "    param_dict.update({'R': trial.suggest_int('R', 1, 10)})\n",
    "\n",
    "    print(\"Trial params\", param_dict)\n",
    "\n",
    "    average_time, aaf = gnn_test(input_path, query_path, queries_num=100, **param_dict, N=5)\n",
    "\n",
    "    return aaf.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    try:\n",
    "        gnns_study = optuna.create_study(study_name='gnns', directions=['minimize', 'minimize'])\n",
    "        gnns_study.optimize(objective_gnns, n_trials=50)\n",
    "        print(\"-------------------- Best trials --------------------\")\n",
    "        trials = sorted(gnns_study.best_trials, key=lambda x: x.values)\n",
    "        for trial in trials:\n",
    "            print(\"Trial no. {}\".format(trial.number))\n",
    "            print(\" Values = {}\".format(trial.values))\n",
    "            print(\" Params = {}\".format(trial.params))\n",
    "        break\n",
    "    except:\n",
    "        print(\"Trial failed, trying again...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gnns_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize GNNS study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(gnns_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(gnns_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(gnns_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(gnns_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(gnns_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimize MRNG\n",
    "\n",
    "To skip logs, click [here](#visualize-mrng-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mrng(trial):\n",
    "    param_dict = {'l': trial.suggest_int('l', 1, 1000)}\n",
    "    param_dict.update({'N': trial.suggest_int('N', 1, param_dict['l'])})\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    average_time, aaf = mrng_test(input_path, query_path, queries_num=100, **param_dict)\n",
    "\n",
    "    return aaf.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    try:\n",
    "        mrng_study = optuna.create_study(study_name='mrng', directions=['minimize', 'minimize'])\n",
    "        mrng_study.optimize(objective_mrng, n_trials=50)\n",
    "        print(\"-------------------- Best trials --------------------\")\n",
    "        trials = sorted(mrng_study.best_trials, key=lambda x: x.values)\n",
    "        for trial in trials:\n",
    "            print(\"Trial no. {}\".format(trial.number))\n",
    "            print(\" Values = {}\".format(trial.values))\n",
    "            print(\" Params = {}\".format(trial.params))\n",
    "        break\n",
    "    except:\n",
    "        print(\"Trial failed, trying again...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mrng_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize MRNG study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(mrng_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(mrng_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(mrng_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(mrng_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(mrng_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimize NSG\n",
    "\n",
    "To skip logs, click [here](#visualize-nsg-study-results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_nsg(trial):\n",
    "    param_dict = {'m' : trial.suggest_int('m', 3, 500),\n",
    "                  'l' : trial.suggest_int('l', 10, 4000),\n",
    "                  'lq': trial.suggest_int('lq', 1, 2000),\n",
    "                  'k' : trial.suggest_int('k', 40, 100)}\n",
    "    \n",
    "    print(\"Trial parameters:\", param_dict)\n",
    "\n",
    "    average_time, aaf = nsg_test(input_path, query_path, queries_num=100, **param_dict, N=5)\n",
    "\n",
    "    return aaf.value, average_time.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(10):\n",
    "    try:\n",
    "        nsg_study = optuna.create_study(study_name='nsg', directions=['minimize', 'minimize'])\n",
    "        nsg_study.optimize(objective_nsg, n_trials=50, n_jobs=-1)\n",
    "        print(\"-------------------- Best trials --------------------\")\n",
    "        trials = sorted(nsg_study.best_trials, key=lambda x: x.values)\n",
    "        for trial in trials:\n",
    "            print(\"Trial no. {}\".format(trial.number))\n",
    "            print(\" Values = {}\".format(trial.values))\n",
    "            print(\" Params = {}\".format(trial.params))\n",
    "        break\n",
    "    except:\n",
    "        print(\"Trial failed, trying again...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nsg_study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.copy(deep=True)\n",
    "df_sorted = df_sorted.dropna(subset=['values_0', 'values_1'])\n",
    "df_sorted = df_sorted.sort_values(by=['values_0', 'values_1'], ascending=[True, True])\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "df_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize NSG study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pareto_front(nsg_study, target_names=['aaf', 'average_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(nsg_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(nsg_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(nsg_study, target = lambda t: t.values[0], target_name = 'aaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(nsg_study, target = lambda t: t.values[1], target_name = 'average_time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
